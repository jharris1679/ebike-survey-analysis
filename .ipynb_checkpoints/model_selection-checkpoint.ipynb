{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Bike Survey Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "\n",
    "Given responses to 23 questions from 2239 people about their demographics, daily transportation details, and opinions on e-bikes, predict if a respondent answers \"No\" when asked if their household has access to a vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "I am approaching this problem as a proof of concept. If this were a more formal research project with business value, I would invest more time feature engineering. In particular, there may be some value in the opinion questions about e-bikes using bike lanes and sidewalks. As a first pass, however, I believe almost all predictive information will be in the demographics and daily transportation details of respondants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Questions\n",
    "\n",
    "#### Which models did you consider? Which Model did you choose and why? How good was it?\n",
    "\n",
    "First of all, when discussing model performance, it is important to note that only 23% of respondents gave the target response (1). This means that a non-classifier that predicts the opposite (0) for every example will be be 77% accurate.\n",
    "\n",
    "My first consideration was logistic regression, because we're dealing with binary classification and it provides coefficients for interpreting the impact of features. It gets a CV score of 77%, which is no better than the simple case discussed above. \n",
    "\n",
    "I moved on to a decision tree, since they are especially good at ignoring irrelevant features, as well as being rather interpretable. It delivers a CV score of 79%, which is not huge improvement but worth noting.\n",
    "\n",
    "I then tried a random forst based on the success of the decision tree, however after experimenting with the number of trees, also the minimum splitting threshhold and depth of the trees, a sinlgle decision tree gave the best results. When using multiple trees, the algorithm would tend toward making no predictions of the target response, so I kept on eye on the confusion matrix to ensure this wasn't happening. No score close to 77% worth reporting.\n",
    "\n",
    "Finally, a I pulled a bit of a trick, using a library that automatically optimizes algorithm pipelines using genetic programming. This method is all force and no finess, but can be a good sanity check; if it can't find some chain of algorithms and a set of parameters that outperforms your best model, your model is maybe okay. I used relatively small numbers for population size and number of generations, though this still took roughly 30 mins to complete. It gave a marginally better CV score of 80.5%. This tells me that there may not be much improvement to be had in terms of model selection and tuning, suggesting that revisiting the features may be worth the effort. That said, I'm running a much larger version in the background just for fun, that will take roughly a day to complete.\n",
    "\n",
    "#### What was the pattern of missing values? Was it random? Could those be inferred from the context?\n",
    "\n",
    "The pattern of missing values was not completely random. For example, there were no nulls in the target field. Nulls seemed to be more common in income and employment fields, though also present in other demographic fields. I don't believe the missing data could be inferred, and would suggest that there are few enough nulls that it may not be worth investing the time on that in this case.\n",
    "\n",
    "#### Which features were significant in predicting the target response?\n",
    "\n",
    "The coefficients of the logistic regression don't exactly agree with the decsion tree, so I will examine the decsion tree since it had better performance. \n",
    "\n",
    "The root node of the tree was based on the frequent_transport fields, which describes which mode of transport respondents use for their day to day commuting. Income is of secondary importance, and then age is also included in the tree. These are the only three features used in the tree. This selection makes some intuitive sense; if you ride your bike everyday and don't have much money, chances are you don't have access to a car.\n",
    "\n",
    "#### If you could re-design the survey for next year, what question(s) would you add or remove in order to improve the precision of the prediction?\n",
    "\n",
    "The biggest thing I would change is the phrasing of the target question. It asks if the respondent's household has access to a car, rather than if they own a car themselves. The ambiguity of this question leaves the possibility that there's a car around, but the respondent isn't using it often. Instead, I would ask directly about car ownership. \n",
    "\n",
    "If it was for some reason not possible to change the target question, I would at least add a question about the number of people living in the household. The more people, the greater chance the car belongs to someone else.\n",
    "\n",
    "Refining the categories for residence location would also help. 'Central Toronto, York, and East York' is far too broad, given that someone living downtown is much less likely to have access to a car that someone living further out.\n",
    "\n",
    "I would also reduce the opportunity for users to enter their own content; many fields had a long tail of custom responses, even the target field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the data, I first loaded it into a database (BigQuery, to be specific). I then pulled frequency counts for each category in each field, to get a sense of which categories needed to be bucketed together. Looking at the data this way, I developed a case statement for each field that gave a number to each category bucket, keeping categories in order where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing was done with sql. see sql/generate_encoded_features.sql\n",
    "data = pd.read_csv(\"data/encoded_features.csv\")\n",
    "# restricting to the most important features doesn't improve anything.\n",
    "#features_to_keep = ['age_range', 'income', 'frequent_transport'] \n",
    "features = data.iloc[:,:10]\n",
    "targets = data.iloc[:,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the ratio of 1s to 0s in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1716\n",
       "1     522\n",
       "Name: vehicle_access, dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.vehicle_access.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23324396782841822"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "522 / (1716+522)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since categories are ordered, grouping by the target variable and looking at the difference in the average category value will give us an idea if one variable is skewed toward or away from the target response. This tabel suggests that income, age_range, frequent_transport, and commute_distance may play an important role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_range</th>\n",
       "      <th>sex</th>\n",
       "      <th>health</th>\n",
       "      <th>education</th>\n",
       "      <th>income</th>\n",
       "      <th>employment</th>\n",
       "      <th>residence_location</th>\n",
       "      <th>commute_distance</th>\n",
       "      <th>commute_time</th>\n",
       "      <th>frequent_transport</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vehicle_access</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.027389</td>\n",
       "      <td>1.260490</td>\n",
       "      <td>1.836830</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>4.560606</td>\n",
       "      <td>4.096737</td>\n",
       "      <td>1.406177</td>\n",
       "      <td>2.923660</td>\n",
       "      <td>2.299534</td>\n",
       "      <td>2.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.657088</td>\n",
       "      <td>1.350575</td>\n",
       "      <td>1.827586</td>\n",
       "      <td>0.471264</td>\n",
       "      <td>3.436782</td>\n",
       "      <td>3.796935</td>\n",
       "      <td>1.072797</td>\n",
       "      <td>2.469349</td>\n",
       "      <td>2.264368</td>\n",
       "      <td>1.965517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age_range       sex    health  education    income  \\\n",
       "vehicle_access                                                       \n",
       "0                3.027389  1.260490  1.836830   0.575758  4.560606   \n",
       "1                2.657088  1.350575  1.827586   0.471264  3.436782   \n",
       "\n",
       "                employment  residence_location  commute_distance  \\\n",
       "vehicle_access                                                     \n",
       "0                 4.096737            1.406177          2.923660   \n",
       "1                 3.796935            1.072797          2.469349   \n",
       "\n",
       "                commute_time  frequent_transport  \n",
       "vehicle_access                                    \n",
       "0                   2.299534            2.409091  \n",
       "1                   2.264368            1.965517  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('vehicle_access').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we train some models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787202380952\n",
      "[[478  34]\n",
      " [109  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.93      0.87       512\n",
      "          1       0.60      0.32      0.42       160\n",
      "\n",
      "avg / total       0.76      0.79      0.76       672\n",
      "\n",
      "0.773480397207\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg_predicted = log_reg.predict(X_test)\n",
    "scores = cross_val_score(LogisticRegression(), features, targets, scoring='accuracy', cv=5)\n",
    "print(metrics.accuracy_score(y_test, log_reg_predicted))\n",
    "print(metrics.confusion_matrix(y_test, log_reg_predicted))\n",
    "print(metrics.classification_report(y_test, log_reg_predicted))\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802083333333\n",
      "[[462  48]\n",
      " [ 85  77]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.91      0.87       510\n",
      "          1       0.62      0.48      0.54       162\n",
      "\n",
      "avg / total       0.79      0.80      0.79       672\n",
      "\n",
      "0.790466677041\n"
     ]
    }
   ],
   "source": [
    "# decision tree classifier\n",
    "dtree = DecisionTreeClassifier(criterion='entropy', min_samples_split=300, max_depth=4)\n",
    "dtree.fit(X_train, y_train)\n",
    "dtree_predicted = dtree.predict(X_test)\n",
    "scores = cross_val_score(DecisionTreeClassifier(criterion='entropy', min_samples_split=300, max_depth=4),\n",
    "                         features, targets, scoring='accuracy', cv=5)\n",
    "print(metrics.accuracy_score(y_test, dtree_predicted))\n",
    "print(metrics.confusion_matrix(y_test, dtree_predicted))\n",
    "print(metrics.classification_report(y_test, dtree_predicted))\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766369047619\n",
      "[[479  33]\n",
      " [124  36]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.94      0.86       512\n",
      "          1       0.52      0.23      0.31       160\n",
      "\n",
      "avg / total       0.73      0.77      0.73       672\n",
      "\n",
      "0.752940665792\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 5\n",
    "min_samples_split = 30\n",
    "max_depth = 5\n",
    "\n",
    "rand_forest = RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                     criterion='entropy', min_samples_split=min_samples_split, \n",
    "                                     max_depth=max_depth)\n",
    "rand_forest.fit(X_train, y_train)\n",
    "rand_forest_predicted = rand_forest.predict(X_test)\n",
    "scores = cross_val_score(RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                                criterion='entropy', min_samples_split=min_samples_split, \n",
    "                                                max_depth=max_depth),\n",
    "                         features, targets, scoring='accuracy', cv=5)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, rand_forest_predicted))\n",
    "print(metrics.confusion_matrix(y_test, rand_forest_predicted))\n",
    "print(metrics.classification_report(y_test, rand_forest_predicted))\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay just for fun, I want to try TPOT, a \"tool that optimizes machine learning pipelines using genetic programming.\"\n",
    "https://github.com/rhiever/tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh/miniconda3/lib/python3.6/site-packages/deap/creator.py:141: RuntimeWarning: A class named 'FitnessMulti' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n",
      "/Users/josh/miniconda3/lib/python3.6/site-packages/deap/creator.py:141: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n",
      "Optimization Progress:  10%|▉         | 150/1575 [01:38<26:19,  1.11s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.7969394192222381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  14%|█▍        | 225/1575 [02:59<26:02,  1.16s/pipeline]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.7982153395331801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  19%|█▉        | 300/1575 [04:18<24:21,  1.15s/pipeline]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.8007651451944404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  24%|██▍       | 375/1575 [05:39<18:14,  1.10pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.8007651451944404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  29%|██▊       | 450/1575 [06:56<17:52,  1.05pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.8007651451944404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  33%|███▎      | 525/1575 [08:16<14:34,  1.20pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6 - Current best internal CV score: 0.8007651451944404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  38%|███▊      | 600/1575 [09:25<10:49,  1.50pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7 - Current best internal CV score: 0.8014000529089762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  43%|████▎     | 675/1575 [11:09<15:25,  1.03s/pipeline]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 8 - Current best internal CV score: 0.8026820781017887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  48%|████▊     | 750/1575 [12:56<08:31,  1.61pipeline/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 9 - Current best internal CV score: 0.8033230906981951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  52%|█████▏    | 825/1575 [14:01<22:35,  1.81s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 10 - Current best internal CV score: 0.8033230906981951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  57%|█████▋    | 900/1575 [15:21<10:32,  1.07pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 11 - Current best internal CV score: 0.8033230906981951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  62%|██████▏   | 975/1575 [16:50<08:12,  1.22pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 12 - Current best internal CV score: 0.8033230906981951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  67%|██████▋   | 1050/1575 [17:53<09:53,  1.13s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 13 - Current best internal CV score: 0.8033230906981951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  71%|███████▏  | 1125/1575 [19:27<06:28,  1.16pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 14 - Current best internal CV score: 0.8052379886449197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  76%|███████▌  | 1200/1575 [20:44<05:27,  1.15pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 15 - Current best internal CV score: 0.8052379886449197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  81%|████████  | 1275/1575 [22:09<06:07,  1.23s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 16 - Current best internal CV score: 0.8052379886449197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  86%|████████▌ | 1350/1575 [23:28<01:42,  2.19pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 17 - Current best internal CV score: 0.8052379886449197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  90%|█████████ | 1425/1575 [24:32<02:31,  1.01s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 18 - Current best internal CV score: 0.8058769662807025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  95%|█████████▌| 1500/1575 [25:49<01:48,  1.44s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 19 - Current best internal CV score: 0.8058769662807025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 20 - Current best internal CV score: 0.8058769662807025\n",
      "\n",
      "Best pipeline: LogisticRegression(GradientBoostingClassifier(SelectFwe(input_matrix, alpha=0.023), learning_rate=0.01, max_depth=4, max_features=0.75, min_samples_leaf=8, min_samples_split=20, n_estimators=100, subsample=0.05), C=10.0, dual=False, penalty=l2)\n",
      "0.797619047619\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=20, population_size=75, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the brute force genetic approach didn't improve much on the simple decision tree. Let's have a look at the logistic regression coefficeints, and also export a png of our decision tree. That file will be included in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_range</th>\n",
       "      <th>sex</th>\n",
       "      <th>health</th>\n",
       "      <th>education</th>\n",
       "      <th>income</th>\n",
       "      <th>employment</th>\n",
       "      <th>residence_location</th>\n",
       "      <th>commute_distance</th>\n",
       "      <th>commute_time</th>\n",
       "      <th>frequent_transport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.371284</td>\n",
       "      <td>0.157661</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>-0.12394</td>\n",
       "      <td>-0.354943</td>\n",
       "      <td>0.086925</td>\n",
       "      <td>-0.552794</td>\n",
       "      <td>-0.452932</td>\n",
       "      <td>0.153797</td>\n",
       "      <td>-0.17436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_range       sex    health  education    income  employment  \\\n",
       "0  -0.371284  0.157661  0.003825   -0.12394 -0.354943    0.086925   \n",
       "\n",
       "   residence_location  commute_distance  commute_time  frequent_transport  \n",
       "0           -0.552794         -0.452932      0.153797            -0.17436  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the coefficients\n",
    "coef = pd.DataFrame(log_reg.coef_)\n",
    "coef.columns = features.columns\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tree(tree, feature_names):\n",
    "    \"\"\"Create tree png using graphviz.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-learn DecsisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    \"\"\"\n",
    "    with open(\"d_tree.dot\", 'w') as f:\n",
    "        export_graphviz(tree, out_file=f,\n",
    "                        feature_names=feature_names)\n",
    "\n",
    "    command = [\"dot\", \"-Tpng\", \"d_tree.dot\", \"-o\", \"d_tree.png\"]\n",
    "    try:\n",
    "        subprocess.check_call(command)\n",
    "    except:\n",
    "        exit(\"Could not run dot, ie graphviz, to \"\n",
    "             \"produce visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates png file in current directory\n",
    "visualize_tree(dtree, features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknoledgements\n",
    "\n",
    "Creation of PNG: http://chrisstrelioff.ws/sandbox/2015/06/08/decision_trees_in_python_with_scikit_learn_and_pandas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
